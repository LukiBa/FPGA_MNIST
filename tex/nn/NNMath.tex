\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{xfrac}
\usepackage{todonotes}

\title{Neural Network Implementation Draft}
\date{October 2019}

\begin{document}
\maketitle

\section{Matrix Calculus}

\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
            $y$ & $\frac{\partial}{\partial x} y$ \\
        \midrule
            $Ax$     & $A^T$ \\
            $x^T A$  & $A$   \\
            $x^T x$  & $2x$  \\  
            $x^T Ax$ & $Ax + A^Tx$  \\          
        \bottomrule
    \end{tabular}
    \caption{Useful derivatives equations}
\end{table}

\subsection{Chain Rule for Matrix Calculus}

The chain rule for a vectors is similar to the chain rule for scalars. Except the order is important. For $\mathbf{z} = f(\mathbf{y})$ and $\mathbf{y} = g(\mathbf{x}) $ the chain rule is:
\begin{equation}
    \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}}     \frac{\partial \mathbf{z}}{\partial \mathbf{y}}
\end{equation}

\section{Example: 3 Layer Fully Connected Neural Network}

For the input $x$ the neural network which is described by its weights $W$, its biases $b$ and the activation functions $g(t)$. The network has $L_1$ neurons in the first layer, $L_2$ neurons in the second layer and $L_3$ neurons in the final layer.

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
            Layer & Weights & Bias \\
        \midrule
            1     & [L1 nx] & [L1 1] \\
            2     & [L2 L1] & [L2 1] \\
            3     & [ny L2] & [ny 1] \\
        \bottomrule
    \end{tabular}
    \caption{Dimensions of the weight and bias matrices}
\end{table}

\begin{align*}
    z_1 &= W_1 x + b_1 \\
    a_1 &= f(z_1)      \\
    z_2 &= W_2 a_1 + b_2 \\
    a_2 &= f(z_2)      \\
    z_3 &= W_3 a_2 + b_3 \\
    h   &= z_3         \\
\end{align*}

\begin{equation}
    J = \frac{1}{N} \sum_i^N (h(x_i;W,b)-y_i)^2
\end{equation}

\subsection{Backpropagation}

The update rule for gradient descent is
\begin{equation}
    p_{i+1} = p_i + \mu \frac{\partial J}{\partial p_i}  \quad \forall p \in \{ W, b \} 
\end{equation}

The main difficulty here is to calculate the gradient for each parameter in the network, which can easily be several thousands or even million parameters. Here back-propagation is used to efficiently calculate those derivatives. The first step is to differentiate the cost function with respect to an parameter $p$ which can describe an weight or a bias

\begin{equation}
    \frac{\partial J}{\partial p_i}  = \frac{2}{N} \sum_i^N (h(x_i;W,b)-y_i) \frac{\partial h}{\partial p_i} 
\end{equation}

The total list of needed derivatives are: \todo{The order is not correct yet}

\begin{align*}
    \frac{\partial h}{\partial W_3} &=  \frac{\partial h}{\partial z_3} \frac{\partial z_3}{\partial W_3} = a_2 \\ 
    \frac{\partial h}{\partial W_2} &= \frac{\partial h}{\partial a_2}     \frac{\partial a_2}{\partial z_2}     \frac{\partial z_2}{\partial W_2} = W_3^T f'(z_2) a_2 \\
    \frac{\partial h}{\partial W_1} &= \frac{\partial h}{\partial a_2} \frac{\partial a_2}{\partial z_2} \frac{\partial z_2}{\partial a_1} \frac{\partial a_1}{\partial z_1} \frac{\partial z_1}{\partial W_1} = W_2^T W_3^T f'(z_2) f'(z_1)   x  \\
    %
    \frac{\partial h}{\partial b_3} &=  \frac{\partial h}{\partial z_3} \frac{\partial z_3}{\partial b_3}  = 1 \\ 
    \frac{\partial h}{\partial b_2} &= \frac{\partial h}{\partial a_2}     \frac{\partial a_2}{\partial z_2}     \frac{\partial z_2}{\partial b_2} = W_3 f'(z_2)   \\
    \frac{\partial h}{\partial b_1} &= \frac{\partial h}{\partial a_2} \frac{\partial a_2}{\partial z_2} \frac{\partial z_2}{\partial a_1} \frac{\partial a_1}{\partial z_1} \frac{\partial a_1}{\partial b_1} =  W_2^T W_3^T  f'(z_2) f'(z_1)  \\
\end{align*}

Note: Here the equations are used as scalars. Because those are vectors all equations need to be transposed.


Calculations of the delta terms is as follows
\begin{equation}
    \delta^{(4)} = h - y 
\end{equation}
then the next delta value is computed using the update equation
\begin{equation}
    \delta^{(l-1)} = W_i^T \delta^{(l)} .* f'(z) 
\end{equation}
and then the total update term is calculated:
\begin{equation}
    \Delta^{(l)} = \delta^{(l)} a^{(l)}    
\end{equation}

It can be seen that the same derivatives are used more often

\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
            Derivative & Result  \\
        \midrule
            $\sfrac{\partial z_1}{\partial W_1}$     & $ x $ \\
            $\sfrac{\partial z_1}{\partial b_1}$     & $ 1 $ \\
            $\sfrac{\partial a_1}{\partial z_1}$     & $ f'(z_1) $ \\
        \midrule
            $\sfrac{\partial z_2}{\partial a_1}$     & $ W_2 $ \\
            $\sfrac{\partial z_2}{\partial W_2}$     & $ a_1 $ \\
            $\sfrac{\partial z_2}{\partial b_2}$     & $ 1 $ \\
            $\sfrac{\partial a_2}{\partial z_2}$     & $ f'(z_2) $ \\
        \midrule
            $\sfrac{\partial z_3}{\partial a_2}$     & $ W_3 $ \\
            $\sfrac{\partial z_3}{\partial W_3}$     & $ a_2 $ \\
            $\sfrac{\partial z_3}{\partial b_3}$     & $ 1 $ \\
        \bottomrule
    \end{tabular}
    \caption{Calculations of all derivatives of the network}
\end{table}

Here something to note here: $f'(z_i)$ is the derivative of the activation function with respect to $z_i$. As an example, the equation of the ReLU activation function is:
\begin{equation}
    f(t) = \begin{cases}
        t   \quad \text{if} \quad t > 0 \\
        0   \quad \text{else}
    \end{cases}
\end{equation}
The derivative $f'(t)$ is
\begin{equation}
    f'(t) = \begin{cases}
        1   \quad \text{if} \quad t > 0 \\
        0   \quad \text{else}
    \end{cases}
\end{equation}


\section{Example: Convolutional Neural Network}

The two dimensional convolution is defined as:

\begin{equation}
   z(i,j) = (f*g)(i,j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m,n) g(m-i,n-j)
\end{equation}

Compared to the previous example the input data is now not one dimensional but two-dimensional.

Also because the problem tackled here is a classification one, the loss function used is the cross-entropy function:
\begin{equation}
    J = - y .* \log(h) + (1-y) .* \log(1-h)
\end{equation}
where $.*$ is used as the element-wise multiplication. This can be interpreted as:
\begin{equation}
    J = \begin{cases}
        - y_i * \log(h_i)       \quad &\text{if} \quad y_i = 1 \\
        (1 - y_i) * \log(h_i)   \quad &\text{if} \quad y_i = 0
    \end{cases}
\end{equation}
    
Because each image is exclusively in one class, the vector 
\begin{equation}
y = \begin{bmatrix}
    y_1 & y_2 & \cdots & y_n
\end{bmatrix}^T
\end{equation}
consists of all zeros except for a single $1$.

 

\end{document}
